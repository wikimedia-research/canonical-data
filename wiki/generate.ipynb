{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows you to regenerate `wikis.tsv` so that it incorporates any newly-created wikis or data changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import wmfdata as wmf\n",
    "from wmfdata.utils import get_dblist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/home/nshahquinn-wmf/wmfdata-python/wmfdata/mariadb.py:142: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  result = pd.read_sql_query(\n"
     ]
    }
   ],
   "source": [
    "# We start with all.dblist (instead of a sites table) because it excludes deleted wikis\n",
    "wikis = get_dblist(\"all\")\n",
    "wikis = pd.DataFrame(wikis, columns=[\"database_code\"])\n",
    "\n",
    "# Left join to the sites table to get additional basic details\n",
    "sites = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "    site_global_key AS database_code,\n",
    "    CONCAT(TRIM(LEADING \".\" FROM REVERSE(site_domain))) AS domain_name,\n",
    "    site_group AS database_group,\n",
    "    site_language AS language_code\n",
    "FROM sites\n",
    "\"\"\", \"metawiki\")\n",
    "\n",
    "wikis = (\n",
    "    wikis\n",
    "    .merge(sites, how=\"left\", on=\"database_code\", sort=\"True\")\n",
    "    .set_index(\"database_code\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_name</th>\n",
       "      <th>database_group</th>\n",
       "      <th>language_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>database_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [domain_name, database_group, language_code]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for wikis with null data (e.g. newly created wikis not yet added to the sites table)\n",
    "# If there are some, you can add the data manually in the following cell, but it may be more\n",
    "# efficient to just wait a few days for the source data to be added.\n",
    "wikis[wikis.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing data. Remove manual additions once no longer needed\n",
    "extra_wikis = pd.DataFrame([\n",
    "    # Example: (\"gcrwiki\", \"gcr.wikipedia.org\", \"wikipedia\", \"gcr\"),\n",
    "], columns=[\"database_code\", \"domain_name\", \"database_group\", \"language_code\"]\n",
    ").set_index(\"database_code\")\n",
    "\n",
    "wikis.update(extra_wikis, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobile domain names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the mobile URL templates from the production wgMobileUrlTemplate config\n",
    "# variable. As of Aug 2023, the variable is defined in\n",
    "# https://gerrit.wikimedia.org/r/plugins/gitiles/operations/mediawiki-config\n",
    "# /+/refs/heads/master/wmf-config/InitialiseSettings.php\n",
    "#\n",
    "# The templates here are kept in the same format and the same order as in the source\n",
    "# variable, for ease of comparison.\n",
    "default_template = \"{h0}.m.{h1}.{h2}\"\n",
    "other_templates = {\n",
    "    \"mediawikiwiki\": \"m.{h1}.{h2}\",\n",
    "    \"sourceswiki\": \"m.{h0}.{h1}\",\n",
    "    \"wikidatawiki\": \"m.{h1}.{h2}\",\n",
    "    \"wikifunctionswiki\": \"m.{h1}.{h2}\",\n",
    "    # \"wikitech\" is a database group that contains Wikitech (database code \"labswiki\") \n",
    "    # and Test Wikitech (database code \"labtestwiki\")\n",
    "    \"labswiki\": \"\",\n",
    "    \"labtestwiki\": \"\"\n",
    "}\n",
    "\n",
    "def derive_mobile_domain(row):\n",
    "    split_domain = row[\"domain_name\"].split(\".\")\n",
    "    \n",
    "    h_parts = {}\n",
    "    # Assign the parts of the split domain to the names used in the templates (\"h0\", \"h1\", etc.)\n",
    "    for i, p in enumerate(split_domain):\n",
    "        h_parts[f\"h{i}\"] = p\n",
    "    \n",
    "    for db_code, template in other_templates.items():\n",
    "        # The 'name' is the row's index\n",
    "        if row.name == db_code:\n",
    "            if template:\n",
    "                return template.format(**h_parts)\n",
    "            # An empty template means the desktop URL is used unmodified\n",
    "            else:\n",
    "                return \".\".join(h_parts.values())\n",
    "                \n",
    "    return default_template.format(**h_parts)\n",
    "\n",
    "wikis[\"mobile_domain_name\"] = wikis.apply(derive_mobile_domain, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_urls = [\n",
    "    \"https://raw.githubusercontent.com/wikimedia/mediawiki-extensions-cldr/master/CldrNames/CldrNamesEn.php\",\n",
    "    \"https://raw.githubusercontent.com/wikimedia/mediawiki-extensions-cldr/master/LocalNames/LocalNamesEn.php\"\n",
    "]\n",
    "\n",
    "def get_lang_names(url):\n",
    "    r = requests.get(url)\n",
    "    m = re.search(r\"languageNames = (\\[[\\s\\S]+?\\])\", r.text)\n",
    "    php_ln = m.group(1)\n",
    "    \n",
    "    json_ln = php_ln\n",
    "    repl = [\n",
    "        # Convert from PHP array format to JSON\n",
    "        (\" =>\", \":\"),\n",
    "        (\"\\[\", \"{\"),\n",
    "        (\"\\]\", \"}\"),\n",
    "        # Trailing commas will cause problems\n",
    "        (\",\\n}\", \"\\n}\"),\n",
    "        # ...so will single quotes\n",
    "        (\"'\", '\"'),\n",
    "        # Now, escaped double quotes (formerly escaped single quotes) can just be normal single quotes\n",
    "        ('\\\\\\\\\"', \"'\"),\n",
    "        # ...and comments\n",
    "        (r\"/\\*[\\s\\S]*?\\*/\", \"\"),\n",
    "        (r\"#(.*?)\\n\", \"\"),\n",
    "        # One hack to deal with a single quote in a language name that was double quoted instead of \n",
    "        # escaped in the file\n",
    "        ('O\"odham', \"O'odham\"),\n",
    "    ]\n",
    "    for old, new in repl:\n",
    "        json_ln = re.sub(old, new, json_ln)\n",
    "    \n",
    "    py_ln = json.loads(json_ln)\n",
    "    return py_ln\n",
    "\n",
    "langs = {}\n",
    "for url in lang_urls:\n",
    "    langs.update(get_lang_names(url))\n",
    "\n",
    "wikis[\"language_name\"] = wikis[\"language_code\"].apply(langs.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'btm',\n",
       " 'dga',\n",
       " 'diq',\n",
       " 'igl',\n",
       " 'kus',\n",
       " 'map-bms',\n",
       " 'nah',\n",
       " 'pih',\n",
       " 'simple',\n",
       " 'szy',\n",
       " 'tay'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for wikis with null language names since some are not included in CLDR\n",
    "null_lang_wikis = wikis[wikis[\"language_name\"].isna()].copy()\n",
    "set(null_lang_wikis[\"language_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing language names. Remove manual additions once no longer needed. \n",
    "extra_langs = {\n",
    "    \"btm\": \"Mandailing\",\n",
    "    \"dga\": \"Dagaare\",\n",
    "    \"diq\": \"Zazaki\",\n",
    "    \"igl\": \"Igala\",\n",
    "    \"kus\": \"Kusaal\",\n",
    "    \"map-bms\": \"Banyumasan\",\n",
    "    \"nah\": \"Nahuatl\",\n",
    "    \"pih\": \"Norfuk-Pitkern\",\n",
    "    \"simple\": \"Simple English\",\n",
    "    \"szy\": \"Sakizaya\",\n",
    "    \"tay\": \"Atayal\"\n",
    "}\n",
    "\n",
    "null_lang_wikis[\"language_name\"] = null_lang_wikis[\"language_code\"].apply(extra_langs.get)\n",
    "wikis.update(null_lang_wikis, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status and access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed = get_dblist(\"closed\")\n",
    "private = get_dblist(\"private\")\n",
    "fishbowl = get_dblist(\"fishbowl\")\n",
    "editability_private = private + fishbowl\n",
    "\n",
    "wikis = (\n",
    "    wikis.assign(\n",
    "        status=lambda df: np.where(df.index.isin(closed), \"closed\", \"open\"),\n",
    "        visibility=lambda df: np.where(df.index.isin(private), \"private\", \"public\"),\n",
    "        editability=lambda df: np.where(df.index.isin(editability_private), \"private\", \"public\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_names = requests.get(\n",
    "    \"https://raw.githubusercontent.com/wikimedia/mediawiki-extensions-WikimediaMessages/master/\" +\n",
    "    \"i18n/wikimediaprojectnames/en.json\"\n",
    ").text\n",
    "\n",
    "replacements = [\n",
    "    (r'\"project-localized-name-(\\w*)\":', r'\"\\1\":'),\n",
    "    (r'\"@metadata\": \\[.*\\],', '')\n",
    "]\n",
    "\n",
    "for old, new in replacements:\n",
    "    project_names = re.sub(old, new, project_names)\n",
    "\n",
    "project_names = json.loads(project_names)\n",
    "project_names = pd.DataFrame.from_dict(\n",
    "    project_names, \n",
    "    orient=\"index\", \n",
    "    columns=[\"english_name\"]\n",
    ").rename_axis(\"database_code\")\n",
    "\n",
    "wikis = wikis.merge(project_names, on=\"database_code\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arbcom_cswiki',\n",
       " 'electcomwiki',\n",
       " 'hiwikimedia',\n",
       " 'projectcomwiki',\n",
       " 'ptwikimedia',\n",
       " 'romdwikimedia',\n",
       " 'techconductwiki',\n",
       " 'testcommonswiki',\n",
       " 'wbwikimedia',\n",
       " 'yuewiktionary']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for wikis with null English names since some might not be included in WikimediaMessages' en.json\n",
    "null_name_wikis = wikis[wikis[\"english_name\"].isna()].copy()\n",
    "null_name_wikis.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we figure out what to name wikis for which we do not have names?\n",
    "\n",
    "1. The type of the wiki might be identifiable through the database code. For example, all Wikipedia wikis end with `wiki`, and and all affiliate and user group wikis end with `wikimedia`. For Wikipedias, there's the [List of Wikipedias](https://meta.wikimedia.org/wiki/List_of_Wikipedias), and for other wikis there's the [Complete list of Wikimedia projects](https://meta.wikimedia.org/wiki/Special:MyLanguage/Complete_list_of_Wikimedia_projects).\n",
    "2. Chapter, user group, and affiliate wikis tend to be named in either a way that ends with \"Wikimedians User Group\", or named after a country (e.g. \"Wikimedia Portugal\"). Visiting the wiki and using translation services can give an idea of what the name is, and some of these wikis are in English.\n",
    "3. Committee wikis might have a database code that ends with `comwiki`. Again, finding the wikis URL and visiting it can give an idea of what the name is.\n",
    "4. For most of these there are already existing wikis in the list, so querying it will give you an idea of what the naming scheme is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing English names. Remove manual additions once no longer needed. \n",
    "extra_names = {\n",
    "    \"arbcom_cswiki\" : \"Czech Wikipedia Arbitration Committee\",\n",
    "    'electcomwiki' : \"Wikimedia Foundation Elections Committee\",\n",
    "    'hiwikimedia' : \"Hindi Wikimedians User Group\",\n",
    "    'projectcomwiki' : \"Project Grants Committee\",\n",
    "    'ptwikimedia' : \"Wikimedia Portugal\",\n",
    "    'romdwikimedia' : \"Wikimedians of Romania and Moldova User Group\",\n",
    "    'techconductwiki' : \"Code of Conduct Committee for Technical Spaces\",\n",
    "    'testcommonswiki' : \"Commons Test Wiki\",\n",
    "    'wbwikimedia' : \"West Bengal Wikimedians User Group \",\n",
    "    'yuewiktionary' : \"Cantonese Wiktionary\"\n",
    "}\n",
    "\n",
    "null_name_wikis[\"english_name\"] = null_name_wikis.index.map(extra_names.get)\n",
    "wikis.update(null_name_wikis, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikis.to_csv(\"wikis.tsv\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
